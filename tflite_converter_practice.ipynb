{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TOG4hXetJ7f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers.experimental import AdamW\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, BatchNormalization, Activation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# windows tensorflow only recognizes CUDA GPU at version 2.10.0\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYYz_1r_MP8J",
    "outputId": "7a8db38e-887b-4307-c70e-dc814a136a38"
   },
   "outputs": [],
   "source": [
    "# Load training and test datasets\n",
    "train_set = pd.read_csv('fashion-mnist_train.csv')\n",
    "train_images = train_set.drop('label', axis=1).values.reshape(-1, 28, 28)\n",
    "train_labels = train_set['label'].values\n",
    "\n",
    "test_set = pd.read_csv('fashion-mnist_test.csv')\n",
    "test_images = test_set.drop('label', axis=1).values.reshape(-1, 28, 28)\n",
    "test_labels = test_set['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHAzN7TFkQwt",
    "outputId": "fe051b9e-1378-4067-bfa1-65330ff2f456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oeWCmrLskQzO"
   },
   "outputs": [],
   "source": [
    "# Convert labels to one-hot vectors\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2x1fnjbEkBDV",
    "outputId": "1c4a8b23-8fc8-4856-d260-173eccf1e117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs = 30, batch size = 128, learning rate = 0.01\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "POCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "alpha = 0.01\n",
    "print('epochs = {}, batch size = {}, learning rate = {}'.format(EPOCHS, BATCH_SIZE, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GGwAJV-ZniRZ"
   },
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, 3, strides=2, padding='same',\n",
    "                 input_shape=(28,28,1),\n",
    "                 kernel_initializer='he_normal',\n",
    "                 bias_initializer='zeros'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(16, 2, strides=1, padding='same',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 bias_initializer='zeros'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, kernel_initializer='glorot_uniform',\n",
    "                bias_initializer='zeros'))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer = AdamW(), metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H84PfHCjq9tg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "469/469 - 8s - loss: 0.5389 - accuracy: 0.8077 - val_loss: 0.3761 - val_accuracy: 0.8673 - 8s/epoch - 18ms/step\n",
      "Epoch 2/30\n",
      "469/469 - 3s - loss: 0.3771 - accuracy: 0.8646 - val_loss: 0.3241 - val_accuracy: 0.8855 - 3s/epoch - 5ms/step\n",
      "Epoch 3/30\n",
      "469/469 - 2s - loss: 0.3411 - accuracy: 0.8760 - val_loss: 0.2954 - val_accuracy: 0.8968 - 2s/epoch - 5ms/step\n",
      "Epoch 4/30\n",
      "469/469 - 2s - loss: 0.3170 - accuracy: 0.8855 - val_loss: 0.2881 - val_accuracy: 0.8989 - 2s/epoch - 5ms/step\n",
      "Epoch 5/30\n",
      "469/469 - 2s - loss: 0.3005 - accuracy: 0.8910 - val_loss: 0.2773 - val_accuracy: 0.9004 - 2s/epoch - 5ms/step\n",
      "Epoch 6/30\n",
      "469/469 - 3s - loss: 0.2849 - accuracy: 0.8974 - val_loss: 0.2782 - val_accuracy: 0.8987 - 3s/epoch - 5ms/step\n",
      "Epoch 7/30\n",
      "469/469 - 2s - loss: 0.2760 - accuracy: 0.8997 - val_loss: 0.2625 - val_accuracy: 0.9059 - 2s/epoch - 5ms/step\n",
      "Epoch 8/30\n",
      "469/469 - 3s - loss: 0.2689 - accuracy: 0.9022 - val_loss: 0.2710 - val_accuracy: 0.9032 - 3s/epoch - 6ms/step\n",
      "Epoch 9/30\n",
      "469/469 - 3s - loss: 0.2619 - accuracy: 0.9039 - val_loss: 0.2501 - val_accuracy: 0.9083 - 3s/epoch - 5ms/step\n",
      "Epoch 10/30\n",
      "469/469 - 3s - loss: 0.2571 - accuracy: 0.9044 - val_loss: 0.2479 - val_accuracy: 0.9122 - 3s/epoch - 5ms/step\n",
      "Epoch 11/30\n",
      "469/469 - 3s - loss: 0.2490 - accuracy: 0.9093 - val_loss: 0.2511 - val_accuracy: 0.9113 - 3s/epoch - 6ms/step\n",
      "Epoch 12/30\n",
      "469/469 - 3s - loss: 0.2451 - accuracy: 0.9109 - val_loss: 0.2495 - val_accuracy: 0.9110 - 3s/epoch - 6ms/step\n",
      "Epoch 13/30\n",
      "469/469 - 3s - loss: 0.2407 - accuracy: 0.9108 - val_loss: 0.2464 - val_accuracy: 0.9117 - 3s/epoch - 5ms/step\n",
      "Epoch 14/30\n",
      "469/469 - 3s - loss: 0.2374 - accuracy: 0.9128 - val_loss: 0.2407 - val_accuracy: 0.9145 - 3s/epoch - 6ms/step\n",
      "Epoch 15/30\n",
      "469/469 - 3s - loss: 0.2325 - accuracy: 0.9154 - val_loss: 0.2387 - val_accuracy: 0.9137 - 3s/epoch - 6ms/step\n",
      "Epoch 16/30\n",
      "469/469 - 3s - loss: 0.2290 - accuracy: 0.9162 - val_loss: 0.2329 - val_accuracy: 0.9165 - 3s/epoch - 6ms/step\n",
      "Epoch 17/30\n",
      "469/469 - 3s - loss: 0.2276 - accuracy: 0.9155 - val_loss: 0.2337 - val_accuracy: 0.9172 - 3s/epoch - 6ms/step\n",
      "Epoch 18/30\n",
      "469/469 - 3s - loss: 0.2240 - accuracy: 0.9184 - val_loss: 0.2336 - val_accuracy: 0.9162 - 3s/epoch - 6ms/step\n",
      "Epoch 19/30\n",
      "469/469 - 3s - loss: 0.2203 - accuracy: 0.9197 - val_loss: 0.2356 - val_accuracy: 0.9169 - 3s/epoch - 6ms/step\n",
      "Epoch 20/30\n",
      "469/469 - 3s - loss: 0.2213 - accuracy: 0.9190 - val_loss: 0.2285 - val_accuracy: 0.9178 - 3s/epoch - 6ms/step\n",
      "Epoch 21/30\n",
      "469/469 - 3s - loss: 0.2155 - accuracy: 0.9211 - val_loss: 0.2315 - val_accuracy: 0.9181 - 3s/epoch - 6ms/step\n",
      "Epoch 22/30\n",
      "469/469 - 2s - loss: 0.2156 - accuracy: 0.9208 - val_loss: 0.2302 - val_accuracy: 0.9168 - 2s/epoch - 5ms/step\n",
      "Epoch 23/30\n",
      "469/469 - 3s - loss: 0.2124 - accuracy: 0.9214 - val_loss: 0.2259 - val_accuracy: 0.9189 - 3s/epoch - 6ms/step\n",
      "Epoch 24/30\n",
      "469/469 - 3s - loss: 0.2129 - accuracy: 0.9222 - val_loss: 0.2249 - val_accuracy: 0.9171 - 3s/epoch - 5ms/step\n",
      "Epoch 25/30\n",
      "469/469 - 2s - loss: 0.2106 - accuracy: 0.9216 - val_loss: 0.2314 - val_accuracy: 0.9168 - 2s/epoch - 5ms/step\n",
      "Epoch 26/30\n",
      "469/469 - 3s - loss: 0.2082 - accuracy: 0.9232 - val_loss: 0.2259 - val_accuracy: 0.9209 - 3s/epoch - 6ms/step\n",
      "Epoch 27/30\n",
      "469/469 - 3s - loss: 0.2083 - accuracy: 0.9230 - val_loss: 0.2386 - val_accuracy: 0.9136 - 3s/epoch - 6ms/step\n",
      "Epoch 28/30\n",
      "469/469 - 3s - loss: 0.2035 - accuracy: 0.9253 - val_loss: 0.2222 - val_accuracy: 0.9209 - 3s/epoch - 6ms/step\n",
      "Epoch 29/30\n",
      "469/469 - 3s - loss: 0.2034 - accuracy: 0.9247 - val_loss: 0.2295 - val_accuracy: 0.9180 - 3s/epoch - 6ms/step\n",
      "Epoch 30/30\n",
      "469/469 - 3s - loss: 0.2036 - accuracy: 0.9243 - val_loss: 0.2252 - val_accuracy: 0.9193 - 3s/epoch - 5ms/step\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "%%time\n",
    "history = model.fit(train_images, train_labels, validation_data=(test_images, test_labels),\n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "# Save model as savedmodel file\n",
    "model.save('fashion_mnist_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload model\n",
    "load_model = keras.models.load_model('fashion_mnist_model')\n",
    "float_model_result = load_model.predict(test_images).argmax(axis=1)\n",
    "float_model_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 28, 28, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 16)        2064      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                31370     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,950\n",
      "Trainable params: 33,852\n",
      "Non-trainable params: 98\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "load_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Training Quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive quantization on all model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(train_images.reshape(-1,28,28,1)).batch(1).take(100)\n",
    "    for input_value in dataset:\n",
    "        yield [np.float32(input_value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize with representative int8 range\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('fashion_mnist_model')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tflite model\n",
    "with open('fashion_mnist_tflite_model', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate with tflite interpreter\n",
    "interpreter = tf.lite.Interpreter('fashion_mnist_tflite_model')\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "input_shape = input_details['shape']\n",
    "scale, zero_point = input_details[\"quantization\"]\n",
    "int_model_result = np.array([])\n",
    "for image in test_images:\n",
    "    input_data = image / scale + zero_point\n",
    "    input_data = np.int8(input_data.reshape(input_shape))\n",
    "\n",
    "    interpreter.set_tensor(input_details['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details['index'])[0]\n",
    "    int_model_result = np.append(int_model_result, output_data.argmax())\n",
    "int_model_result.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize-Aware Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize-aware finetuning on dense layers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization.quantization.keras as tfmot\n",
    "from keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark dense layers to be quantized during training\n",
    "def apply_dense_quantizer(layer):\n",
    "    if isinstance(layer, Dense):\n",
    "        return tfmot.quantize_annotate_layer(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 28, 28, 1)        4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 16)        2064      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " quant_flatten (QuantizeWrap  (None, 3136)             1         \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrappe  (None, 10)               31375     \n",
      " rV2)                                                            \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,956\n",
      "Trainable params: 33,852\n",
      "Non-trainable params: 104\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Apply quantize blocks to marked layers\n",
    "annotated_model = clone_model(load_model, clone_function=apply_dense_quantizer)\n",
    "quant_aware_model = tfmot.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 - 6s - loss: 0.2031 - accuracy: 0.9257 - val_loss: 0.2350 - val_accuracy: 0.9164 - 6s/epoch - 12ms/step\n",
      "Epoch 2/20\n",
      "469/469 - 3s - loss: 0.2044 - accuracy: 0.9236 - val_loss: 0.2216 - val_accuracy: 0.9228 - 3s/epoch - 6ms/step\n",
      "Epoch 3/20\n",
      "469/469 - 3s - loss: 0.2002 - accuracy: 0.9270 - val_loss: 0.2190 - val_accuracy: 0.9217 - 3s/epoch - 6ms/step\n",
      "Epoch 4/20\n",
      "469/469 - 3s - loss: 0.1979 - accuracy: 0.9266 - val_loss: 0.2195 - val_accuracy: 0.9227 - 3s/epoch - 6ms/step\n",
      "Epoch 5/20\n",
      "469/469 - 3s - loss: 0.1977 - accuracy: 0.9276 - val_loss: 0.2194 - val_accuracy: 0.9204 - 3s/epoch - 7ms/step\n",
      "Epoch 6/20\n",
      "469/469 - 3s - loss: 0.1948 - accuracy: 0.9286 - val_loss: 0.2290 - val_accuracy: 0.9177 - 3s/epoch - 6ms/step\n",
      "Epoch 7/20\n",
      "469/469 - 3s - loss: 0.1943 - accuracy: 0.9279 - val_loss: 0.2265 - val_accuracy: 0.9179 - 3s/epoch - 6ms/step\n",
      "Epoch 8/20\n",
      "469/469 - 3s - loss: 0.1951 - accuracy: 0.9268 - val_loss: 0.2233 - val_accuracy: 0.9220 - 3s/epoch - 6ms/step\n",
      "Epoch 9/20\n",
      "469/469 - 3s - loss: 0.1931 - accuracy: 0.9283 - val_loss: 0.2204 - val_accuracy: 0.9214 - 3s/epoch - 6ms/step\n",
      "Epoch 10/20\n",
      "469/469 - 3s - loss: 0.1913 - accuracy: 0.9295 - val_loss: 0.2254 - val_accuracy: 0.9194 - 3s/epoch - 6ms/step\n",
      "Epoch 11/20\n",
      "469/469 - 3s - loss: 0.1901 - accuracy: 0.9296 - val_loss: 0.2197 - val_accuracy: 0.9218 - 3s/epoch - 6ms/step\n",
      "Epoch 12/20\n",
      "469/469 - 3s - loss: 0.1933 - accuracy: 0.9295 - val_loss: 0.2277 - val_accuracy: 0.9167 - 3s/epoch - 6ms/step\n",
      "Epoch 13/20\n",
      "469/469 - 3s - loss: 0.1918 - accuracy: 0.9284 - val_loss: 0.2170 - val_accuracy: 0.9231 - 3s/epoch - 6ms/step\n",
      "Epoch 14/20\n",
      "469/469 - 3s - loss: 0.1912 - accuracy: 0.9301 - val_loss: 0.2168 - val_accuracy: 0.9222 - 3s/epoch - 6ms/step\n",
      "Epoch 15/20\n",
      "469/469 - 3s - loss: 0.1877 - accuracy: 0.9304 - val_loss: 0.2181 - val_accuracy: 0.9218 - 3s/epoch - 6ms/step\n",
      "Epoch 16/20\n",
      "469/469 - 3s - loss: 0.1883 - accuracy: 0.9297 - val_loss: 0.2199 - val_accuracy: 0.9214 - 3s/epoch - 6ms/step\n",
      "Epoch 17/20\n",
      "469/469 - 3s - loss: 0.1885 - accuracy: 0.9300 - val_loss: 0.2199 - val_accuracy: 0.9234 - 3s/epoch - 6ms/step\n",
      "Epoch 18/20\n",
      "469/469 - 3s - loss: 0.1871 - accuracy: 0.9298 - val_loss: 0.2254 - val_accuracy: 0.9219 - 3s/epoch - 6ms/step\n",
      "Epoch 19/20\n",
      "469/469 - 3s - loss: 0.1870 - accuracy: 0.9313 - val_loss: 0.2226 - val_accuracy: 0.9228 - 3s/epoch - 6ms/step\n",
      "Epoch 20/20\n",
      "469/469 - 3s - loss: 0.1866 - accuracy: 0.9316 - val_loss: 0.2206 - val_accuracy: 0.9224 - 3s/epoch - 6ms/step\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "# Start quantize-aware training, which finetunes model with controlled precision points\n",
    "%%time\n",
    "quant_aware_model.compile(loss='categorical_crossentropy', optimizer = AdamW(), metrics =['accuracy'])\n",
    "history = quant_aware_model.fit(train_images, train_labels, validation_data=(test_images, test_labels),\n",
    "                                epochs=20, batch_size=128, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, flatten_layer_call_fn, flatten_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n",
      "c:\\Users\\Justin\\Anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
     ]
    }
   ],
   "source": [
    "# Apply quantization now that quantize blocks are ready\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fashion_mnist_tflite_qat_model', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter('fashion_mnist_tflite_qat_model')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_shape = input_details['shape']\n",
    "qat_model_result = np.array([])\n",
    "for image in test_images:\n",
    "    input_data = np.int8(input_data.reshape(input_shape))\n",
    "    interpreter.set_tensor(input_details['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details['index'])[0]\n",
    "    qat_model_result = np.append(int_model_result, output_data.argmax())\n",
    "qat_model_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32: 91.93\n",
      "Naive Post-Training Quantize Int8: 91.9\n",
      "Quantize Aware Finetuning Int8: 91.9\n"
     ]
    }
   ],
   "source": [
    "# Accuracy comparisons\n",
    "labels = test_labels.argmax(axis=1)\n",
    "print(\"Float32:\", (np.sum(labels==float_model_result)*100)/len(labels))\n",
    "print(\"Naive Post-Training Quantize Int8:\", (np.sum(labels==int_model_result)*100)/len(labels))\n",
    "print(\"Quantize Aware Finetuning Int8:\", (np.sum(labels==qat_model_result[:-1])*100)/len(labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
